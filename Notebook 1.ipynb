{"cells":[{"cell_type":"code","source":"import torch #PyTorch kütüphanesini içe aktarıyoruz.\ntorch.manual_seed(1) #Sonuçların tekrarlanabilir olması için manuel bir tohum değeri belirliyoruz.\nx = torch.tensor([[1, 2, 3],[4,5,6]],dtype=torch.float) #3 sütuna sahip 2 boyutlu bir tensör tanımlıyoruz ve içinde kayan noktalı sayılar bulunuyor.\ninput=x.shape[1] #Girdi özelliklerinin sayısını, girdi tensörünün ikinci boyutundan elde ediyoruz.\nhidden_size=50 #Ağdaki gizli birimlerin sayısını belirliyoruz.\noutput = 1 #Ağdaki çıkış birimlerinin sayısını belirliyoruz.\nweight1 = torch.randn(input, hidden_size, dtype=torch.float) #İlk katmanın ağırlık matrisini rastgele değerlerle başlatıyoruz.\nweight2 = torch.randn(hidden_size,output,dtype=torch.float) #İkinci katmanın ağırlık matrisini rastgele değerlerle başlatıyoruz.\nbias1 = torch.randn(hidden_size,dtype=torch.float) #İlk katmanın bias vektörünü rastgele değerlerle başlatıyoruz.\nbias2 = torch.randn(output,dtype=torch.float) #İkinci katmanın bias vektörünü rastgele değerlerle başlatıyoruz.\ndef sigmoid_aktivasyon(x): #Sigmoid aktivasyon fonksiyonunu tanımlıyoruz.\n  return 1 / (1 + torch.exp(-x))\ndef tanh_aktivasyon(x): #Hiperbolik tanjant (tanh) aktivasyon fonksiyonunu tanımlıyoruz.\n  return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\nh = tanh_aktivasyon(torch.mm(x,weight1) + bias1) #İlk katmanın çıktısını hesaplayarak, girdi tensörü ile ilk katmanın ağırlık matrisinin çarpımına bias vektörünü ekleyip, tanh aktivasyon fonksiyonuna uyguluyoruz.\ny=sigmoid_aktivasyon(torch.mm(h,weight2)+bias2) #İkinci katmanın çıktısını hesaplayarak, ilk katmanın çıktısı ile ikinci katmanın ağırlık matrisinin çarpımına bias vektörünü ekleyip, sigmoid aktivasyon fonksiyonuna uyguluyoruz. Bu sonuç, ağın nihai çıktısıdır.\ny","metadata":{"cell_id":"e65178c20c664f1c9e8f9d5c8ac8b755","source_hash":"591dbaf2","execution_start":1681452242777,"execution_millis":2172,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"tensor([[0.9998],\n        [0.9984]])"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import torch #PyTorch kütüphanesini içe aktarıyoruz.\ntorch.manual_seed(190401101) #Sonuçların tekrarlanabilir olması için manuel bir tohum değeri belirliyoruz.\nx = torch.tensor([[1, 2, 3],[4,5,6]],dtype=torch.float) #3 sütuna sahip 2 boyutlu bir tensör tanımlıyoruz ve içinde kayan noktalı sayılar bulunuyor.\ninput=x.shape[1] #Girdi özelliklerinin sayısını, girdi tensörünün ikinci boyutundan elde ediyoruz.\nhidden_size=50 #Ağdaki gizli birimlerin sayısını belirliyoruz.\noutput = 1 #Ağdaki çıkış birimlerinin sayısını belirliyoruz.\nweight1 = torch.randn(input, hidden_size, dtype=torch.float) #İlk katmanın ağırlık matrisini rastgele değerlerle başlatıyoruz.\nweight2 = torch.randn(hidden_size,output,dtype=torch.float) #İkinci katmanın ağırlık matrisini rastgele değerlerle başlatıyoruz.\nbias1 = torch.randn(hidden_size,dtype=torch.float) #İlk katmanın bias vektörünü rastgele değerlerle başlatıyoruz.\nbias2 = torch.randn(output,dtype=torch.float) #İkinci katmanın bias vektörünü rastgele değerlerle başlatıyoruz.\ndef sigmoid_aktivasyon(x): #Sigmoid aktivasyon fonksiyonunu tanımlıyoruz.\n  return 1 / (1 + torch.exp(-x))\ndef tanh_aktivasyon(x): #Hiperbolik tanjant (tanh) aktivasyon fonksiyonunu tanımlıyoruz.\n  return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\nh = tanh_aktivasyon(torch.mm(x,weight1) + bias1) #İlk katmanın çıktısını hesaplayarak, girdi tensörü ile ilk katmanın ağırlık matrisinin çarpımına bias vektörünü ekleyip, tanh aktivasyon fonksiyonuna uyguluyoruz.\ny=sigmoid_aktivasyon(torch.mm(h,weight2)+bias2) #İkinci katmanın çıktısını hesaplayarak, ilk katmanın çıktısı ile ikinci katmanın ağırlık matrisinin çarpımına bias vektörünü ekleyip, sigmoid aktivasyon fonksiyonuna uyguluyoruz. Bu sonuç, ağın nihai çıktısıdır.\ny","metadata":{"cell_id":"c23f52689566451db2c153d89441d531","source_hash":"b508cf67","execution_start":1681452248356,"execution_millis":13,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"tensor([[0.3878],\n        [0.9689]])"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=78c90822-6279-4f56-bf61-eb12710f3890' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"f6bcac24709d4ab3bccf81a130447ae2","deepnote_execution_queue":[]}}